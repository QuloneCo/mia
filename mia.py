# -*- coding: utf-8 -*-
"""Mia

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gVhkBL4i-WF6-EMe83ERJO9aSyOFttzQ
"""

pip install transformers

import torch
import torch.nn as nn


class sLSTMCell(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(sLSTMCell, self).__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size

        # Parameters for input, forget, and output gates
        self.w_i = nn.Parameter(torch.Tensor(hidden_size, input_size))
        self.w_f = nn.Parameter(torch.Tensor(hidden_size, input_size))
        self.w_o = nn.Parameter(torch.Tensor(hidden_size, input_size))
        self.w_z = nn.Parameter(torch.Tensor(hidden_size, input_size))

        self.r_i = nn.Parameter(
            torch.Tensor(hidden_size, hidden_size)
        )
        self.r_f = nn.Parameter(
            torch.Tensor(hidden_size, hidden_size)
        )
        self.r_o = nn.Parameter(
            torch.Tensor(hidden_size, hidden_size)
        )
        self.r_z = nn.Parameter(
            torch.Tensor(hidden_size, hidden_size)
        )

        self.b_i = nn.Parameter(torch.Tensor(hidden_size))
        self.b_f = nn.Parameter(torch.Tensor(hidden_size))
        self.b_o = nn.Parameter(torch.Tensor(hidden_size))
        self.b_z = nn.Parameter(torch.Tensor(hidden_size))

        self.sigmoid = nn.Sigmoid()

        # Initialize parameters
        self.reset_parameters()

    def reset_parameters(self):
        nn.init.xavier_uniform_(self.w_i)
        nn.init.xavier_uniform_(self.w_f)
        nn.init.xavier_uniform_(self.w_o)
        nn.init.xavier_uniform_(self.w_z)

        nn.init.orthogonal_(self.r_i)
        nn.init.orthogonal_(self.r_f)
        nn.init.orthogonal_(self.r_o)
        nn.init.orthogonal_(self.r_z)

        nn.init.zeros_(self.b_i)
        nn.init.zeros_(self.b_f)
        nn.init.zeros_(self.b_o)
        nn.init.zeros_(self.b_z)

    def forward(self, x, states):
        h_prev, c_prev, n_prev, m_prev = states

        i_tilda = (

            + torch.matmul(self.r_i, h_prev)
            + self.b_i
        )
        f_tilda = (
            torch.matmul(self.w_f, x)
            + torch.matmul(self.r_f, h_prev)
            + self.b_f
        )
        o_tilda = (
            torch.matmul(self.w_o, x)
            + torch.matmul(self.r_o, h_prev)
            + self.b_o
        )
        z_tilda = (
            torch.matmul(self.w_z, x)
            + torch.matmul(self.r_z, h_prev)
            + self.b_z
        )

        i_t = torch.exp(i_tilda)
        f_t = self.sigmoid(
            f_tilda
        )  # Choose either sigmoid or exp based on context

        # Stabilizer state update
        m_t = torch.max(torch.log(f_t) + m_prev, torch.log(i_t))

        # Stabilized gates
        i_prime = torch.exp(torch.log(i_t) - m_t)
        f_prime = torch.exp(torch.log(f_t) + m_prev - m_t)

        c_t = f_prime * c_prev + i_prime * torch.tanh(z_tilda)
        n_t = f_prime * n_prev + i_prime

        c_hat = c_t / n_t
        h_t = self.sigmoid(o_tilda) * torch.tanh(c_hat)

        return h_t, (h_t, c_t, n_t, m_t)


class sLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers):
        super(sLSTM, self).__init__()
        self.layers = nn.ModuleList(
            [
                sLSTMCell(
                    input_size if i == 0 else hidden_size, hidden_size
                )
                for i in range(num_layers)
            ]
        )

    def forward(self, x, initial_states=None):
        batch_size, seq_len, _ = x.size()
        if initial_states is None:
            initial_states = [
                (
                    torch.zeros(
                        batch_size, self.layers[0].hidden_size
                    ),
                    torch.zeros(
                        batch_size, self.layers[0].hidden_size
                    ),
                    torch.zeros(
                        batch_size, self.layers[0].hidden_size
                    ),
                    torch.zeros(
                        batch_size, self.layers[0].hidden_size
                    ),
                )
                for _ in self.layers
            ]

        outputs = []
        current_states = initial_states

        for t in range(seq_len):
            x_t = x[:, t, :]
            new_states = []
            for layer, state in zip(self.layers, current_states):
                h_t, new_state = layer(x_t, state)
                new_states.append(new_state)
                x_t = h_t  # Pass the output to the next layer
            outputs.append(h_t.unsqueeze(1))
            current_states = new_states

        outputs = torch.cat(
            outputs, dim=1
        )  # Concatenate on the time dimension
        return outputs, current_states


x = torch.randn(1, 10, 64)
model = sLSTM(64, 128, 2)

import requests

def connect_dataset(dataset_name, **kwargs):
    # Получаем URL датасета
    url = {
        "ImageNet": "https://image-net.org/api/text/imagenet.synset.geturls?wnid=n02127808",
        "COCO": "https://cocodataset.org/zips/coco2014_trainval2014.zip",
        "OpenImages": "https://storage.googleapis.com/openimages/2019/validation/image_%06d.jpg",
        "Google Landmarks": "https://storage.googleapis.com/google-landmarks/v1/landmark_labels.json",
        "Google Street View": "https://www.googleapis.com/streetview/metadata/locations.json",
        "YouTube": "https://www.googleapis.com/youtube/v3/channels",
        "Books100M": "https://storage.googleapis.com/books/v1/volumes/%s.json",
        "Wikipedia": "https://en.wikipedia.org/wiki/%s",
        "News articles": "https://www.bbc.com/news/%s",
        "Social media posts": "https://www.twitter.com/%s/status/%s",
        "Open-source code": "https://github.com/%s/%s",
        "Patents": "https://patents.google.com/patent/US%s.html",
        "Technical documentation": "https://docs.python.org/3/library/%s.html",
        "Legal documents": "https://www.law.cornell.edu/uscode/text/%s",
        "Medical images": "https://www.kaggle.com/datasets/nih-chest-xrays/chest-xrays",
        "Scientific images": "https://www.nasa.gov/multimedia/imagegallery/",
        "Satellite images": "https://planet.com/products/planetscope",
        "ImageNet22K": "https://image-net.org/download/ILSVRC2012/devkit.tar.gz",
    }.get(dataset_name)

    # Если датасет не является общедоступным, то запрашиваем его у пользователя
    if url is None:
        url = input("Введите URL-адрес датасета:")

    # Получаем данные датасета
    response = requests.get(url)
    if response.status_code == 200:
        data = response.content
    else:
        raise ValueError("Ошибка при получении данных датасета: %s" % response.status_code)

    # Возвращаем данные датасета
    return data


def read_file(data):
    return data

import torch
from transformers import AutoModel, AutoModelForSequenceClassification
from sklearn.metrics import f1_score

# Define the custom attention layer
class AttentionLayer(torch.nn.Module):
    def __init__(self, num_heads, embed_dim=768):
        super().__init__()
        self.attention = torch.nn.MultiheadAttention(
            num_heads=num_heads,
            embed_dim=embed_dim,
            bias=False)

    def forward(self, query, key, value):
        attention, _ = self.attention(query, key, value, attention_mask=None)
        return attention

# Define the loss function
def loss_fn(predictions, labels):
    return FocalLoss(reduction="mean")(predictions, labels)

# Define the optimizer
def optimizer_fn(parameters):
    return AdaBelief(parameters, lr=learning_rate)

# Define the activation function
def activation_fn(x):
    return torch.nn.ReLU()(x)

# Define the encoder
class Encoder(torch.nn.Module):
    def __init__(self, input_dim, hidden_dim):
        super().__init__()
        self.linear1 = torch.nn.Linear(input_dim, hidden_dim)
        self.activation = activation_fn

    def forward(self, x):
        x = self.linear1(x)
        x = self.activation(x)
        return x

# Define the decoder
class Decoder(torch.nn.Module):
    def __init__(self, hidden_dim, output_dim):
        super().__init__()
        self.linear1 = torch.nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        x = self.linear1(x)
        return x

# Create the transformer model
class CustomTransformer(torch.nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_heads):
        super().__init__()
        self.encoder = Encoder(input_dim, hidden_dim)
        self.decoder = Decoder(hidden_dim, output_dim)
        self.attention = AttentionLayer(num_heads)

    def forward(self, x):
        x = self.encoder(x)
        x = self.attention(x, x, x)
        x = self.decoder(x)
        return x

# Create the model
transformer_model = CustomTransformer(
    input_dim=768, hidden_dim=1024, output_dim=2, num_heads=16
)
# Define the custom attention layer
class AttentionLayer(torch.nn.Module):
    def __init__(self, num_heads, embed_dim=768):
        super().__init__()
        self.attention = torch.nn.MultiheadAttention(
            num_heads=num_heads,
            embed_dim=embed_dim,
            bias=False)

    def forward(self, query, key, value):
        attention, _ = self.attention(query, key, value, attention_mask=None)
        return attention
        dropout = 0.1
# Define the loss function
def loss_fn(predictions, labels):
    return FocalLoss(reduction="mean")(predictions, labels)

# Define the optimizer
def optimizer_fn(parameters):
    return AdaBelief(parameters, lr=learning_rate)

# Define the activation function
def activation_fn(x):
    return torch.nn.ReLU()(x)

# Define the encoder
class Encoder(torch.nn.Module):
    def __init__(self, input_dim, hidden_dim):
        super().__init__()
        self.linear1 = torch.nn.Linear(input_dim, hidden_dim)
        self.activation = activation_fn

    def forward(self, x):
        x = self.linear1(x)
        x = self.activation(x)
        return x

# Define the decoder
class Decoder(torch.nn.Module):
    def __init__(self, hidden_dim, output_dim):
        super().__init__()
        self.linear1 = torch.nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        x = self.linear1(x)
        return x

# Create the transformer model
class CustomTransformer(torch.nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_heads):
        super().__init__()
        self.encoder = Encoder(input_dim, hidden_dim)
        self.decoder = Decoder(hidden_dim, output_dim)
        self.attention = AttentionLayer(num_heads)

    def forward(self, x):
        x = self.encoder(x)
        x = self.attention(x, x, x)
        x = self.decoder(x)
        return x
# Create the model
transformer_model = CustomTransformer(
    input_dim=768,
    hidden_dim=1024,
    output_dim=2,
    num_heads=16
)



# ... Дальнейшие операции с моделью ...
import requests
import re

def read_link(link):
    # Разбиваем ссылку на отдельные компоненты
    protocol, domain, port, path = re.match(
        r"(http|https)://(.+):(\d+)(/.*)", link
    ).groups()

    # Подключаемся к ресурсу
    response = requests.get(f"{protocol}://{domain}{path}")

    # Извлечем информацию из ресурса
    if response.status_code == 200:
        data = response.content
        return data
    else:
        return None
import tensorflow as tf

def read_file(filename):
    # Открываем файл в режиме чтения
    file_handle = tf.io.gfile.GFile(filename, "rb")

    # Читаем содержимое файла
    data = file_handle.read()

    # Возвращаем содержимое файла
    return data
    import requests
from bs4 import BeautifulSoup

def read_link(link):
    # Разбиваем ссылку на отдельные компоненты
    protocol, domain, port, path = re.match(
        r"(http|https)://(.+):(\d+)(/.*)", link
    ).groups()

    # Подключаемся к ресурсу
    response = requests.get(f"{protocol}://{domain}{path}")

    # Извлечем информацию из ресурса
    if response.status_code == 200:
        soup = BeautifulSoup(response.content, "html.parser")
        title = soup.title.text
        content = soup.find("div", class_="content").text
        return title, content
    else:
        return None
def read_link(link):
    # Разбиваем ссылку на отдельные компоненты
    protocol, domain, port, path = re.match(
        r"(http|https)://(.+):(\d+)(/.*)", link
    ).groups()

    # Подключаемся к ресурсу
    response = requests.get(f"{protocol}://{domain}{path}")

    # Проверяем, является ли ссылка действительной
    if not response.status_code == 200:
        return None

    # Проверяем, вернулся ли запрос `requests.get()` статус-код 200
    if response.status_код != 200:
        return None

    # Извлечем информацию из ресурса
    soup = BeautifulSoup(response.content, "html.parser")
    title = soup.title.text
    content = soup.find("div", class_="content").text
    return title, content

import requests
import torch
from transformers import AutoModel, AutoModelForSequenceClassification
from sklearn.metrics import f1_score

# Define the function to read data from a link
def read_link(link):
    # Получаем данные датасета
    response = requests.get(link)



# Connect to the dataset
data = read_link("https://image-net.org/api/text/imagenet.synset.geturls?wnid=n02127808")



# Create the model
transformer_model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased")

# Train the model

import torch
import torch.nn as nn
import torch.nn.functional as F

class CNN(nn.Module):
    def __init__(self, num_classes):
        super(CNN, self).__init__()

        # Количество слоев
        self.num_layers = 100

        # Размер ядра
        self.kernel_sizes = [5, 9, 5, 5, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]

        # Шаг свертки
        self.strides = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]

        # Количество фильтров
        self.num_filters = [128, 256, 512, 512, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024]

        # Активационная функция
        self.activation = F.relu

        # Тип пулинга
        self.pooling = nn.MaxPool2d(4, 4)

        # Количество нейронов в полносвязных слоях
        self.fc_sizes = [4096, 4096]

        # Активационная функция
        self.fc_activation = F.softmax

        # Создадим слои сети
        for i in range(self.num_layers):
            if i == 0:
                self.conv1 = nn.Conv2d(3, self.num_filters[i], self.kernel_sizes[i], self.strides[i])
            else:
                self.conv2 = nn.Conv2d(self.num_filters[i - 1], self.num_filters[i], self.kernel_sizes[i], self.strides[i])

        self.fc1 = nn.Linear(self.num_filters[-1] * 14 * 14, self.fc_sizes[0])
        self.fc2 = nn.Linear(self.fc_sizes[0], num_classes)

    def forward(self, x):
        x = self.conv1(x)
        x = self.activation(x)
        x = self.pooling(x)

        for i in range(1, self.num_layers):
            x = self.conv2(x)
            x = self.activation(x)
            x = self.pooling(x)

        x = x.view(-1, self.num_filters[-1] * 14 * 14)
        x = self.fc1(x)
        x = self.activation(x)
        x = self.fc2(x)
        return x

import torch
import torch.nn as nn
import torch.nn.functional as F

class GenerativeTransformer(nn.Module):
    """
    Генеративная трансформерная модель для создания видео, изображений, презентаций, 3D-объектов и таблиц Excel.

    Parameters
    ----------
    num_classes: int
        Количество классов, которые может генерировать модель.
    input_size: int
        Размер входных данных.
    hidden_size: int
        Размер скрытого слоя.
    num_layers: int
        Количество слоев в модели.

    Attributes
    ----------
    encoder: nn.TransformerEncoder
        Энкодер модели.
    decoder: nn.TransformerDecoder
        Декодер модели.
    output_layer: nn.Linear
        Выходной слой модели.
    """

    def __init__(self, num_classes, input_size, hidden_size, num_layers):
        super(GenerativeTransformer, self).__init__()

        self.num_classes = num_classes
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        # Энкодер
        self.encoder = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(
                self.hidden_size * 5,
                num_heads=32,
                dim_feedforward=self.hidden_size * 16,
                dropout=0.2,
            ),
            num_layers=self.num_layers,
        )

        # Декодер
        self.decoder = nn.TransformerDecoder(
            nn.TransformerDecoderLayer(
                self.hidden_size * 5,
                num_heads=32,
                dim_feedforward=self.hidden_size * 16,
                dropout=0.2,
            ),
            num_layers=self.num_layers,
        )

        # Выходной слой
        self.output_layer = nn.Linear(self.hidden_size * 5, self.num_classes)

    def forward(self, x):
        """
        Кодирует и декодирует входные данные.

        Parameters
        ----------
        x: torch.Tensor
            Входные данные.

        Returns
        -------
        torch.Tensor
            Выходные данные.
        """

        # Кодирование
        x = x.view(-1, self.input_size)
        x = self.encoder(x)

        # Декодирование
        x = self.decoder(x)

        # Выход
        x = self.output_layer(x)
        return x

def generate_prompt(model, input_image, target_type):
    # Получим размер входного изображения
    input_size = input_image.size()

    # Создадим пустой промпт
    prompt = ""

    # Если входное изображение является изображением
    if target_type == "image":
        # Добавим в промпт информацию о размере изображения
        prompt += f"Размер изображения: {input_size[1]}x{input_size[0]}"

        # Добавим в промпт описание изображения
        prompt += f"Описание изображения: "
        prompt += input_image

    # Если входное изображение является видео
    elif target_type == "video":
        # Добавим в промпт информацию о количестве кадров в видео
        prompt += f"Количество кадров в видео: {input_size[0]}"

        # Добавим в промпт описание первого кадра видео
        prompt += f"Описание первого кадра видео: "
        prompt += input_image[0]

    # Если входное изображение является 3D объектом
    elif target_type == "3d_object":
        # Добавим в промпт описание 3D объекта
        prompt += f"Описание 3D объекта: нарисуй кота"
        prompt += input_image

    # Если входное изображение является презентацией
    elif target_type == "presentation":
        # Добавим в промпт описание презентации
        prompt += f"Описание презентации: презентацию по рыбе"
        prompt += input_image

    # Если входное изображение является таблицей Excel
    elif target_type == "excel_table":
        # Добавим в промпт описание таблицы Excel
        prompt += f"Описание таблицы Excel: таблицу зарплат компании газпром "
        prompt += input_image

    return prompt

def generate_prompt(text):
    # Преобразуем текст в нижний регистр
    text = text.lower()

    # Удаляем пунктуацию
    text = re.sub(r"[^\w\s]", "", text)

    # Заменяем пробелы на символы подчеркивания
    text = text.replace(" ", "_")

    # Добавляем префикс и суффикс к промпту
    prompt = "prompt: " + text + " |||"

    return prompt

import numpy as np

def split_array(arr):
    return arr[arr > 0], arr[arr <= 0]

my_array = np.array([1, -2, 3, -4, 5])

positive_elements, negative_elements = split_array(my_array)

print("Положительные элементы: ", positive_elements)
print("негативные элементы: ", negative_elements)